{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Semantic Segmentation for Autonomous Driving\n",
    "\n",
    "This notebook introduces the fundamentals of semantic segmentation in the context of autonomous driving. We'll explore basic concepts, load sample data, and implement a simple segmentation pipeline.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the role of semantic segmentation in autonomous driving\n",
    "- Load and visualize road scene images with annotations\n",
    "- Implement basic image preprocessing for segmentation\n",
    "- Evaluate segmentation performance using IoU metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Semantic Segmentation\n",
    "\n",
    "Semantic segmentation assigns a class label to every pixel in an image. In autonomous driving, common classes include:\n",
    "\n",
    "- **Road**: Drivable surface\n",
    "- **Vehicle**: Cars, trucks, buses\n",
    "- **Person**: Pedestrians, cyclists\n",
    "- **Traffic Sign**: Road signs and signals\n",
    "- **Sky**: Background sky region\n",
    "- **Building**: Structures and buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class colors for visualization\n",
    "CLASS_COLORS = {\n",
    "    0: [128, 64, 128],   # Road - purple\n",
    "    1: [244, 35, 232],   # Sidewalk - pink\n",
    "    2: [70, 70, 70],     # Building - dark gray\n",
    "    3: [102, 102, 156],  # Wall - blue-gray\n",
    "    4: [190, 153, 153],  # Fence - light brown\n",
    "    5: [153, 153, 153],  # Pole - gray\n",
    "    6: [250, 170, 30],   # Traffic sign - orange\n",
    "    7: [220, 220, 0],    # Traffic light - yellow\n",
    "    8: [107, 142, 35],   # Vegetation - green\n",
    "    9: [152, 251, 152],  # Terrain - light green\n",
    "    10: [70, 130, 180],  # Sky - blue\n",
    "    11: [220, 20, 60],   # Person - red\n",
    "    12: [255, 0, 0],     # Rider - bright red\n",
    "    13: [0, 0, 142],     # Car - dark blue\n",
    "    14: [0, 0, 70],      # Truck - darker blue\n",
    "    15: [0, 60, 100],    # Bus - blue\n",
    "    16: [0, 80, 100],    # Train - blue\n",
    "    17: [0, 0, 230],     # Motorcycle - bright blue\n",
    "    18: [119, 11, 32]    # Bicycle - dark red\n",
    "}\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole',\n",
    "    'traffic_sign', 'traffic_light', 'vegetation', 'terrain',\n",
    "    'sky', 'person', 'rider', 'car', 'truck', 'bus',\n",
    "    'train', 'motorcycle', 'bicycle'\n",
    "]\n",
    "\n",
    "print(f\"Number of classes: {len(CLASS_NAMES)}\")\n",
    "print(\"Classes:\", CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Loading and Preprocessing\n",
    "\n",
    "Let's create some synthetic road scene data to demonstrate the concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_road_scene(width=640, height=480):\n",
    "    \"\"\"Create a synthetic road scene with basic elements\"\"\"\n",
    "    # Create base image\n",
    "    image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Sky (top portion)\n",
    "    sky_height = height // 3\n",
    "    image[:sky_height, :] = [135, 206, 235]  # Sky blue\n",
    "    mask[:sky_height, :] = 10  # Sky class\n",
    "    \n",
    "    # Road (bottom portion)\n",
    "    road_start = int(height * 0.6)\n",
    "    image[road_start:, :] = [64, 64, 64]  # Dark gray road\n",
    "    mask[road_start:, :] = 0  # Road class\n",
    "    \n",
    "    # Buildings (middle portion)\n",
    "    building_start = sky_height\n",
    "    building_end = road_start\n",
    "    image[building_start:building_end, :width//3] = [139, 69, 19]  # Brown building\n",
    "    mask[building_start:building_end, :width//3] = 2  # Building class\n",
    "    \n",
    "    image[building_start:building_end, 2*width//3:] = [169, 169, 169]  # Gray building\n",
    "    mask[building_start:building_end, 2*width//3:] = 2  # Building class\n",
    "    \n",
    "    # Add some vehicles (simple rectangles)\n",
    "    # Car 1\n",
    "    cv2.rectangle(image, (200, 350), (300, 400), (0, 0, 142), -1)\n",
    "    cv2.rectangle(mask, (200, 350), (300, 400), 13, -1)  # Car class\n",
    "    \n",
    "    # Car 2\n",
    "    cv2.rectangle(image, (450, 380), (550, 430), (255, 0, 0), -1)\n",
    "    cv2.rectangle(mask, (450, 380), (550, 430), 13, -1)  # Car class\n",
    "    \n",
    "    # Add lane markings\n",
    "    cv2.line(image, (width//2-20, road_start), (width//2-20, height), (255, 255, 255), 3)\n",
    "    cv2.line(image, (width//2+20, road_start), (width//2+20, height), (255, 255, 255), 3)\n",
    "    \n",
    "    return image, mask\n",
    "\n",
    "# Create synthetic data\n",
    "sample_image, sample_mask = create_synthetic_road_scene()\n",
    "\n",
    "# Display the synthetic scene\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes[0].imshow(sample_image)\n",
    "axes[0].set_title('Synthetic Road Scene')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(sample_mask, cmap='tab20')\n",
    "axes[1].set_title('Ground Truth Segmentation')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image shape: {sample_image.shape}\")\n",
    "print(f\"Mask shape: {sample_mask.shape}\")\n",
    "print(f\"Unique classes in mask: {np.unique(sample_mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Segmentation Metrics\n",
    "\n",
    "The most important metric for segmentation is Intersection over Union (IoU), also known as the Jaccard Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred_mask, true_mask, num_classes=19):\n",
    "    \"\"\"Calculate IoU for each class and mean IoU\"\"\"\n",
    "    ious = []\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # True positives, false positives, false negatives\n",
    "        tp = np.sum((pred_mask == class_id) & (true_mask == class_id))\n",
    "        fp = np.sum((pred_mask == class_id) & (true_mask != class_id))\n",
    "        fn = np.sum((pred_mask != class_id) & (true_mask == class_id))\n",
    "        \n",
    "        if tp + fp + fn == 0:\n",
    "            iou = 1.0  # Perfect score if class is not present\n",
    "        else:\n",
    "            iou = tp / (tp + fp + fn)\n",
    "        \n",
    "        ious.append(iou)\n",
    "    \n",
    "    mean_iou = np.mean(ious)\n",
    "    return ious, mean_iou\n",
    "\n",
    "def calculate_pixel_accuracy(pred_mask, true_mask):\n",
    "    \"\"\"Calculate overall pixel accuracy\"\"\"\n",
    "    correct_pixels = np.sum(pred_mask == true_mask)\n",
    "    total_pixels = pred_mask.size\n",
    "    return correct_pixels / total_pixels\n",
    "\n",
    "# Create a noisy prediction for demonstration\n",
    "noisy_prediction = sample_mask.copy()\n",
    "# Add some noise\n",
    "noise_mask = np.random.random(sample_mask.shape) < 0.1\n",
    "noisy_prediction[noise_mask] = np.random.randint(0, 19, np.sum(noise_mask))\n",
    "\n",
    "# Calculate metrics\n",
    "ious, mean_iou = calculate_iou(noisy_prediction, sample_mask)\n",
    "pixel_acc = calculate_pixel_accuracy(noisy_prediction, sample_mask)\n",
    "\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "print(f\"Pixel Accuracy: {pixel_acc:.4f}\")\n",
    "\n",
    "# Display per-class IoU\n",
    "plt.figure(figsize=(12, 6))\n",
    "present_classes = np.unique(sample_mask)\n",
    "present_ious = [ious[i] for i in present_classes]\n",
    "present_names = [CLASS_NAMES[i] for i in present_classes]\n",
    "\n",
    "plt.bar(present_names, present_ious)\n",
    "plt.title('Per-Class IoU Scores')\n",
    "plt.ylabel('IoU Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Image Preprocessing\n",
    "\n",
    "Before feeding images to segmentation models, we typically apply preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, target_size=(512, 512)):\n",
    "    \"\"\"Preprocess image for segmentation model\"\"\"\n",
    "    # Resize image\n",
    "    resized = cv2.resize(image, target_size)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    normalized = resized.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Apply ImageNet normalization (common for pretrained models)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    standardized = (normalized - mean) / std\n",
    "    \n",
    "    return standardized\n",
    "\n",
    "def augment_image(image, mask):\n",
    "    \"\"\"Apply data augmentation\"\"\"\n",
    "    # Random horizontal flip\n",
    "    if np.random.random() > 0.5:\n",
    "        image = np.fliplr(image)\n",
    "        mask = np.fliplr(mask)\n",
    "    \n",
    "    # Random brightness adjustment\n",
    "    brightness_factor = np.random.uniform(0.8, 1.2)\n",
    "    image = np.clip(image * brightness_factor, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return image, mask\n",
    "\n",
    "# Demonstrate preprocessing\n",
    "preprocessed = preprocess_image(sample_image)\n",
    "augmented_img, augmented_mask = augment_image(sample_image.copy(), sample_mask.copy())\n",
    "\n",
    "# Visualize preprocessing steps\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(sample_image)\n",
    "axes[0, 0].set_title('Original Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(sample_mask, cmap='tab20')\n",
    "axes[1, 0].set_title('Original Mask')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Preprocessed\n",
    "# Denormalize for visualization\n",
    "denorm = (preprocessed * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406]))\n",
    "denorm = np.clip(denorm, 0, 1)\n",
    "\n",
    "axes[0, 1].imshow(denorm)\n",
    "axes[0, 1].set_title('Preprocessed Image')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(cv2.resize(sample_mask, (512, 512)), cmap='tab20')\n",
    "axes[1, 1].set_title('Resized Mask')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Augmented\n",
    "axes[0, 2].imshow(augmented_img)\n",
    "axes[0, 2].set_title('Augmented Image')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(augmented_mask, cmap='tab20')\n",
    "axes[1, 2].set_title('Augmented Mask')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple Segmentation Algorithm\n",
    "\n",
    "Let's implement a basic color-based segmentation as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_color_segmentation(image):\n",
    "    \"\"\"Simple color-based segmentation\"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    segmentation = np.zeros((h, w), dtype=np.uint8)\n",
    "    \n",
    "    # Convert to HSV for better color segmentation\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    # Sky detection (blue colors in upper part)\n",
    "    sky_mask = (hsv[:, :, 2] > 100) & (hsv[:, :, 1] < 100)  # Bright, low saturation\n",
    "    sky_mask[:h//2, :] = sky_mask[:h//2, :] | (hsv[:h//2, :, 0] > 90) & (hsv[:h//2, :, 0] < 130)\n",
    "    segmentation[sky_mask] = 10  # Sky class\n",
    "    \n",
    "    # Road detection (dark colors in lower part)\n",
    "    road_mask = (hsv[h//2:, :, 2] < 100) & (hsv[h//2:, :, 1] < 50)  # Dark, low saturation\n",
    "    segmentation[h//2:, :][road_mask] = 0  # Road class\n",
    "    \n",
    "    # Vehicle detection (distinct colors)\n",
    "    vehicle_mask = (hsv[:, :, 1] > 150) | ((hsv[:, :, 0] < 15) | (hsv[:, :, 0] > 165))  # High saturation or red/blue\n",
    "    vehicle_mask = vehicle_mask & (hsv[:, :, 2] > 50)  # Not too dark\n",
    "    segmentation[vehicle_mask] = 13  # Car class\n",
    "    \n",
    "    # Building detection (everything else in middle region)\n",
    "    middle_region = slice(h//4, 3*h//4)\n",
    "    building_mask = (segmentation[middle_region, :] == 0)\n",
    "    segmentation[middle_region, :][building_mask] = 2  # Building class\n",
    "    \n",
    "    return segmentation\n",
    "\n",
    "# Apply simple segmentation\n",
    "simple_seg = simple_color_segmentation(sample_image)\n",
    "\n",
    "# Compare with ground truth\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(sample_image)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(sample_mask, cmap='tab20')\n",
    "axes[1].set_title('Ground Truth')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(simple_seg, cmap='tab20')\n",
    "axes[2].set_title('Simple Color Segmentation')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate performance\n",
    "simple_ious, simple_mean_iou = calculate_iou(simple_seg, sample_mask)\n",
    "simple_pixel_acc = calculate_pixel_accuracy(simple_seg, sample_mask)\n",
    "\n",
    "print(f\"Simple Segmentation Mean IoU: {simple_mean_iou:.4f}\")\n",
    "print(f\"Simple Segmentation Pixel Accuracy: {simple_pixel_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix Analysis\n",
    "\n",
    "Let's analyze the segmentation performance using confusion matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, class_names, title='Confusion Matrix'):\n",
    "    \"\"\"Plot confusion matrix for segmentation results\"\"\"\n",
    "    # Flatten arrays\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "    \n",
    "    # Get unique classes present in the data\n",
    "    present_classes = np.unique(np.concatenate([y_true_flat, y_pred_flat]))\n",
    "    present_class_names = [class_names[i] for i in present_classes]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true_flat, y_pred_flat, labels=present_classes)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm_normalized = np.nan_to_num(cm_normalized)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_normalized, \n",
    "                xticklabels=present_class_names,\n",
    "                yticklabels=present_class_names,\n",
    "                annot=True, \n",
    "                fmt='.2f',\n",
    "                cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cm, cm_normalized\n",
    "\n",
    "# Plot confusion matrix for simple segmentation\n",
    "cm, cm_norm = plot_confusion_matrix(sample_mask, simple_seg, CLASS_NAMES, \n",
    "                                   'Simple Color Segmentation Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps and Exercises\n",
    "\n",
    "This notebook provided a foundation for understanding semantic segmentation in autonomous driving. Here are some exercises to extend your learning:\n",
    "\n",
    "### Exercise 1: Improve Color Segmentation\n",
    "- Enhance the simple color segmentation algorithm\n",
    "- Add more sophisticated color ranges\n",
    "- Use morphological operations for noise reduction\n",
    "\n",
    "### Exercise 2: Dataset Analysis\n",
    "- Load real automotive datasets (Cityscapes, KITTI)\n",
    "- Analyze class distributions and imbalances\n",
    "- Implement data augmentation strategies\n",
    "\n",
    "### Exercise 3: Performance Metrics\n",
    "- Implement additional metrics (Dice coefficient, F1-score)\n",
    "- Analyze per-class performance in detail\n",
    "- Compare different segmentation approaches\n",
    "\n",
    "### Exercise 4: Preprocessing Pipeline\n",
    "- Build a complete preprocessing pipeline\n",
    "- Implement various augmentation techniques\n",
    "- Optimize for model training efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of key concepts\n",
    "print(\"Key Concepts Covered:\")\n",
    "print(\"1. Semantic segmentation for autonomous driving\")\n",
    "print(\"2. Class definitions and color coding\")\n",
    "print(\"3. IoU and pixel accuracy metrics\")\n",
    "print(\"4. Image preprocessing and augmentation\")\n",
    "print(\"5. Simple color-based segmentation\")\n",
    "print(\"6. Confusion matrix analysis\")\n",
    "print(\"\\nNext: Proceed to U-Net implementation notebook\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}